# Dynamic Causal Reasoning Framework (DCRF)

## Overview

The **Dynamic Causal Reasoning Framework (DCRF)** is a comprehensive toolkit for causal discovery, analysis, and reasoning in complex systems. The framework combines state-of-the-art causal inference algorithms with practical variable selection and statistical validation techniques to enable robust causal modeling and decision-making.

DCRF provides:

- **Causal Discovery**: Learn causal structures from observational data using FCI (Fast Causal Inference) algorithm
- **Variable Impact Analysis**: Quantify causal effects and rank variables by their importance
- **Smart Variable Selection**: Strategic selection methods balancing coverage and parsimony
- **Temporal Causal Modeling**: Handle time-lagged relationships and temporal constraints
- **Statistical Validation**: Rigorous hypothesis testing and effect size estimation

## Table of Contents

- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Experiments Overview](#experiments-overview)
  - [Causal Variable Analysis Suite](#causal-variable-analysis-suite)
  - [FCI Temporal Ordering Experiments](#fci-temporal-ordering-experiments)
- [Detailed Experiment Logic](#detailed-experiment-logic)
- [Dataset](#dataset)
- [Key Notebooks](#key-notebooks)
- [Citation](#citation)

---

## Project Structure

```
DCRF/
├── README.md                          # This file
├── data/
│   └── ihdp.csv                       # IHDP dataset (Infant Health and Development Program)
├── experiments/
│   ├── experiment_1/                  # Impact value calculation
│   ├── experiment_2/                  # Quadrant division by impact
│   ├── experiment_3/                  # Variable role classification
│   ├── experiment_4/                  # 75% selection rule
│   └── experiment_5/                  # Statistical test application
├── docs/
│   ├── Desc Doc.txt                   # Project description and algorithm choices
│   ├── Lit Survey.txt                 # Literature survey
│   └── walkthrough- Temporal Ordering.md  # FCI experiments walkthrough
├── fci.ipynb                          # FCI algorithm notebook with temporal experiments
├── causal.ipynb                       # Causal analysis notebook
├── dag.ipynb                          # DAG modeling notebook
├── effect_estimation.ipynb            # Effect estimation examples
├── fci_temporal_experiments.py        # Temporal ordering experiment script
├── fci_adjacency_matrix.csv           # FCI-generated PAG adjacency matrix
└── output/                            # Generated outputs and visualizations
```

---

## Installation

### Requirements

- Python 3.8+
- Jupyter Notebook

### Dependencies

Install required packages:

```bash
pip install pandas numpy scipy scikit-learn matplotlib seaborn
pip install causal-learn networkx statsmodels
```

### Quick Setup

```bash
# Clone or navigate to project directory
cd /Volumes/DevDrive/DCRF

# Install dependencies
pip install -r requirements.txt  # if available, or use command above

# Launch Jupyter for interactive exploration
jupyter notebook
```

---

## Quick Start

### Run All Experiments Sequentially

```bash
cd /Volumes/DevDrive/DCRF

# Step 1: Generate FCI graph (run fci.ipynb first)
jupyter notebook fci.ipynb

# Step 2: Run causal variable analysis experiments
cd experiments/experiment_1
python run_experiment_1a.py
python run_experiment_1b.py

cd ../experiment_2
python run_experiment_2a.py
python run_experiment_2b.py

cd ../experiment_3
python run_experiment_3a.py
python run_experiment_3b.py

cd ../experiment_4
python run_experiment_4a.py
python run_experiment_4b.py

cd ../experiment_5
python run_experiment_5a.py
python run_experiment_5b.py
python run_experiment_5c.py
```

---

## Experiments Overview

### Causal Variable Analysis Suite

This suite consists of **five interconnected experiments** that progressively analyze, categorize, and select variables based on their causal impact on outcomes. The experiments use the IHDP dataset and a Partial Ancestral Graph (PAG) generated by the FCI algorithm.

#### Pipeline Flow

```
Input Data (IHDP) + FCI PAG
         ↓
Experiment 1: Calculate Impact Values
         ↓
Experiment 2: Divide into Quadrants
         ↓
Experiment 3: Classify Causal Roles
         ↓
Experiment 4: Select Variables (75% Rule)
         ↓
Experiment 5: Statistical Validation
         ↓
Final Selected Variables + Significance Tests
```

### FCI Temporal Ordering Experiments

A comprehensive evaluation of how temporal ordering constraints affect causal discovery performance, implemented in `fci.ipynb` with three main experiments testing different aspects of temporal knowledge integration.

---

## Detailed Experiment Logic

### Experiment 1: Impact Value Calculation

**Location**: `experiments/experiment_1/`

**Purpose**: Quantify the causal effect (impact) of each variable on the outcome variable (`y_factual`) using multiple methods.

#### Logic & Rationale

Causal impact estimation is fundamental to understanding which variables matter most. However, different methods make different assumptions and capture different aspects of causality:

1. **Linear Regression**: Assumes linear relationships, gives standardized coefficients
2. **Partial Dependence**: Model-agnostic, captures non-linear effects by marginalizing
3. **Do-Calculus (Backdoor Adjustment)**: Theory-grounded, uses PAG to identify valid adjustment sets
4. **Causal Forest**: Data-driven variable importance using Random Forest as proxy

**Experiment 1A** runs all four methods and outputs absolute impact values.

**Experiment 1B** extends this by calculating **confidence intervals** using:

- Bootstrap resampling (distribution-free, robust)
- Analytical confidence intervals (when applicable)

This provides **conservative estimates** via lower bounds, crucial for risk-averse decision-making.

#### Key Outputs

- `results/experiment_1a_results.csv` - Impact values for all methods
- `results/experiment_1b_results.csv` - Impact values with confidence intervals

#### Why Multiple Methods?

**Robustness**: Variables with high impact across multiple methods are more reliable  
**Complementarity**: Each method captures different aspects of causal relationships  
**Validation**: Consistency across methods validates findings

---

### Experiment 2: Quadrant Division by Impact Value

**Location**: `experiments/experiment_2/`

**Purpose**: Divide variables into four quadrants based on **cumulative impact** (not count), ensuring each quadrant contributes ~25% of total impact.

#### Logic & Rationale

Traditional quartiles divide by **count** (equal number of variables per group), but this ignores the fact that a few variables often dominate total impact (**Pareto principle**).

**Value-based division** ensures:

- Q1: Top variables contributing first 25% of total impact (typically few but powerful)
- Q2: Next 25% of impact
- Q3: Next 25% of impact
- Q4: Last 25% of impact (typically many low-impact variables)

#### Algorithm

1. Sort variables by impact (descending)
2. Calculate cumulative impact percentage
3. Assign quadrants:
   - Q1: until cumulative ≥ 25%
   - Q2: until cumulative ≥ 50%
   - Q3: until cumulative ≥ 75%
   - Q4: remaining

**Result**: Quadrants have **different counts** but **equal cumulative impact**.

#### Example

| Variable  | Impact | Cumulative % | Quadrant |
| --------- | ------ | ------------ | -------- |
| treatment | 30     | 30%          | Q1       |
| mu1       | 20     | 50%          | Q2       |
| x16       | 15     | 65%          | Q2       |
| x6        | 10     | 75%          | Q3       |
| others    | 25     | 100%         | Q4       |

**Q1 has 1 variable, Q2 has 2, Q3 has 1, Q4 has many** - reflecting true impact concentration.

#### Validation

**Experiment 2B** validates that each quadrant truly contains ~25% of impact (±2% tolerance) and generates visualizations comparing quadrant distributions across different methods.

---

### Experiment 3: Variable Classification by Causal Role

**Location**: `experiments/experiment_3/`

**Purpose**: Classify variables by their structural role in the causal graph (PAG) and cross-reference with impact quadrants.

#### Logic & Rationale

Not all variables are created equal in causal analysis. Their **structural position** in the causal graph determines their role:

**Variable Roles**:

1. **Independent Variables** (Mediators)

   - On directed path from treatment → outcome
   - Mechanisms through which treatment works
   - Example: If treatment → x → outcome, then x is a mediator

2. **Confounders**

   - Common causes of both treatment AND outcome
   - Must be controlled to avoid bias
   - Example: If x → treatment and x → outcome, then x is a confounder

3. **Edge Variables**

   - Connected to the causal system but not on main pathway
   - May have indirect effects or interactions
   - Important for understanding full system dynamics

4. **Other**
   - Isolated or weakly connected variables
   - May be noise or irrelevant to main causal question

#### Graph Analysis Algorithm

**Experiment 3A**:

1. Identify all directed paths from treatment to outcome
2. Mark variables on these paths as **Independent**
3. Find variables with paths to both treatment AND outcome → **Confounders**
4. Variables with edges but not above → **Edge**
5. Remaining → **Other**

**Experiment 3B**: Creates a **crosstab matrix** combining role × quadrant:

```
           | Independent | Confounder | Edge | Other |
-----------|-------------|------------|------|-------|
Q1 (High)  |    ...      |    ...     | ...  | ...   |
Q2         |    ...      |    ...     | ...  | ...   |
Q3         |    ...      |    ...     | ...  | ...   |
Q4 (Low)   |    ...      |    ...     | ...  | ...   |
```

#### Why This Matters

**High-impact confounders (Q1/Q2)**: Must be controlled - critical for valid inference  
**High-impact mediators (Q1/Q2)**: Understanding mechanisms, intervention targets  
**Low-impact variables (Q3/Q4)**: May be excluded in parsimonious models  
**Role distribution**: Reveals structure of causal system (e.g., many confounders = complex)

---

### Experiment 4: 75% Selection Rule

**Location**: `experiments/experiment_4/`

**Purpose**: Select an **equal number** of variables from Q1, Q2, Q3 (excluding Q4) that collectively cover **75% of total impact**.

#### Logic & Rationale

**Problem**: Naively selecting top-k variables gives all high-impact (Q1) variables, ignoring diversity and potentially missing important context from Q2/Q3.

**Solution**: Enforce **equal representation** from each non-low-impact quadrant while achieving impact coverage target.

#### Algorithm (Experiment 4A)

**Goal**: Find minimum equal count _n_ such that:

- Top _n_ from Q1 + Top _n_ from Q2 + Top _n_ from Q3 ≥ 75% total impact

**Process**:

1. Exclude Q4 entirely (lowest impact)
2. Set target = 0.75 × total_impact
3. For n = 1, 2, 3, ...:
   - Select top _n_ from each of Q1, Q2, Q3
   - Calculate cumulative impact
   - If ≥ target, return selection
4. Return smallest _n_ achieving target

**Example**:

- n=1: Select top 1 from each → 3 variables total → check if ≥75%
- n=2: Select top 2 from each → 6 variables total → check if ≥75%
- Continue until threshold met

#### Why 75%?

**Trade-off**:

- **Higher** (e.g., 90%): More variables, complex models, diminishing returns
- **Lower** (e.g., 50%): Too few variables, may miss critical effects
- **75%**: Balances comprehensiveness with parsimony (based on Pareto principle)

#### Sensitivity Analysis (Experiment 4B)

Tests thresholds: 60%, 70%, 75%, 80%, 90%

For each threshold, tracks:

- Number of variables selected
- Actual impact covered
- Count per quadrant
- Accuracy (distance from target)

**Visualizations**:

1. Variables selected vs. threshold
2. Achieved vs. target coverage
3. Equal count per quadrant
4. Selection precision

**Result**: Validates whether 75% is optimal or if another threshold is preferable for the specific dataset.

---

### Experiment 5: Statistical Test Application

**Location**: `experiments/experiment_5/`

**Purpose**: Apply rigorous statistical tests to establish significance and variance contribution for selected variables from Experiment 4.

#### Logic & Rationale

**Causal impact estimates** (Experiment 1) are point estimates. Statistical tests provide:

- **Significance**: Is the effect real or due to chance?
- **Effect sizes**: Practical importance beyond statistical significance
- **Variance explained**: How much outcome variability is due to each variable?

#### Experiment 5A: Distribution Classification

**Goal**: Determine which statistical test is appropriate for each variable.

**Classification Process**:

1. **Normality Test**: Shapiro-Wilk test (α = 0.05)
2. **Sample Size Check**: Small (n < 30) vs. Large (n ≥ 30)
3. **Combination** → Test category

**Test Selection**:

| Distribution | Sample Size | → Test Applied   | Effect Size   |
| ------------ | ----------- | ---------------- | ------------- |
| Normal       | Small       | → T-test         | Cohen's d     |
| Normal       | Large       | → Z-test         | Cohen's d     |
| Non-normal   | Small       | → Mann-Whitney U | Rank-biserial |
| Non-normal   | Large       | → Wilcoxon       | Rank-biserial |

**Why This Matters**: Using wrong test can lead to invalid p-values and false conclusions.

#### Experiment 5B: Apply Statistical Tests

**Testing Strategy**:

1. For each variable, split data by **median**:
   - Group 1: observations ≤ median
   - Group 2: observations > median
2. Compare **outcome distributions** between groups
3. Apply appropriate test (determined in 5A)
4. Calculate **effect size** (Cohen's d or rank-biserial)

**Interpretation**:

- **Significant p-value (< 0.05)**: Variable creates meaningful outcome difference
- **Large effect size**: Practical importance (Cohen's d ≥ 0.8 or rank-biserial high)

#### Experiment 5C: Variance Contribution Analysis

**Alternative approach** using model comparison (complementary to 5B).

**Method**:

1. Fit **full model**: Y ~ X₁ + X₂ + ... + Xₙ
2. For each variable Xᵢ:
   - Fit **reduced model**: Y ~ all variables except Xᵢ
   - Calculate **ΔR²** = R²_full - R²_reduced
   - Test significance using **F-test**
3. Rank variables by ΔR²

**Why This Complements 5B**:

- 5B: Univariate effect (variable alone)
- 5C: Multivariate contribution (variable in context of others)

Variables with high ΔR² are **irreplaceable** - removing them significantly degrades model fit.

---

## FCI Temporal Ordering Experiments

**Location**: `fci.ipynb` (25 new cells) and `fci_temporal_experiments.py`

**Purpose**: Evaluate how temporal ordering constraints improve causal discovery accuracy when using the FCI (Fast Causal Inference) algorithm.

### Core Concept

**Problem**: Causal discovery from observational data is challenging because:

- Correlation ≠ Causation
- Many graph structures can fit the same statistical patterns
- FCI produces Partial Ancestral Graphs (PAGs) with undirected edges when direction is ambiguous

**Solution**: Incorporate **background knowledge** in the form of temporal ordering:

- If X occurs before Y, then Y cannot cause X (time flows forward)
- This constrains the search space and resolves ambiguities

### Three Experiments

#### Experiment 1.1: Temporal Structure Comparison

**Goal**: Quantify improvement when using temporal constraints vs. not using them.

**Setup**:

1. Generate synthetic DAG with 4 temporal layers (12 variables total)
2. Ensure edges only flow forward in time
3. Generate data from this structure

**Comparison**:

- **Baseline**: Run FCI without temporal constraints
- **Proposed**: Run FCI with temporal constraints (forbid backward edges)

**Metrics**:

- Precision, Recall, F1 Score (edge recovery accuracy)
- Structural Hamming Distance (SHD) - lower is better
- Number of undirected edges - fewer is better (more resolved)

**Expected Result**: Temporal constraints reduce false positives and resolve more edge directions.

#### Experiment 1.2: Noise Level Testing

**Goal**: Test if temporal constraints help more in noisy data.

**Hypothesis**: When data is noisy, statistical relationships are weaker, making background knowledge more valuable.

**Noise Levels**:

- Low: std = 0.1
- Medium: std = 0.5
- High: std = 1.5

**Process**:

1. Generate data with each noise level
2. Run FCI with and without temporal constraints
3. Compare improvement at each noise level

**Visualization**: 4-panel plot showing Precision, Recall, F1, SHD across noise levels for both methods.

**Expected Result**: Benefits of temporal constraints are consistent or amplified in noisy data.

#### Experiment 1.3: Partial Temporal Information

**Goal**: Test scenarios where only some variables have known temporal ordering (realistic real-world scenario).

**Temporal Information Levels**: 0%, 25%, 50%, 75%, 100%

**Setup**:

1. Generate single dataset (for consistency)
2. Randomly mask temporal information for (100-x)% of variables
3. Run FCI with partial constraints

**Key Insight**: Even partial temporal knowledge should help.

**Visualization**: Line plots showing metrics vs. % temporal info available.

**Expected Result**: Gradual improvement as more temporal information is available (no all-or-nothing threshold).

### Implementation Details

**Key Functions**:

- `generate_temporal_dag()`: Creates synthetic DAG with temporal layers
- `run_fci_with_temporal_constraints()`: Runs FCI with optional background knowledge
- `calculate_edge_metrics()`: Computes precision, recall, F1, SHD, undirected edge count
- `mask_temporal_info()`: Randomly masks temporal ordering for sensitivity analysis

**Reproducibility**: All random operations use seed=42 for consistent results.

---

## Dataset

### IHDP (Infant Health and Development Program)

**Location**: `data/ihdp.csv`

**Description**: A widely-used benchmark dataset in causal inference research.

**Variables**:

- **Treatment**: Binary intervention (0/1)
- **Covariates**: ~25 variables including demographics, medical history, socioeconomic factors
- **Outcome**: `y_factual` - observed outcome

**Use Case**: Evaluate causal effect of intervention on infant development.

**Size**: 747 observations

**Ground Truth**: Known treatment assignment mechanism (gold standard for validation).

---

## Key Notebooks

### `fci.ipynb`

- FCI algorithm implementation
- Generates PAG (Partial Ancestral Graph)
- Contains all three temporal ordering experiments
- Outputs: `fci_adjacency_matrix.csv`, `fci_edges.txt`, visualizations

### `causal.ipynb`

- Causal analysis explorations
- Effect estimation examples
- Visualization of causal relationships

### `dag.ipynb`

- DAG modeling and visualization
- Structural analysis
- Path finding and d-separation

### `effect_estimation.ipynb`

- Various causal effect estimation methods
- Comparison of estimators
- Sensitivity analysis

---

## Citation

### Algorithms & Methods

- **FCI Algorithm**: Spirtes, P., Glymour, C., & Scheines, R. (2000). _Causation, Prediction, and Search_. MIT Press.
- **Backdoor Adjustment**: Pearl, J. (2009). _Causality: Models, Reasoning, and Inference_. Cambridge University Press.
- **causal-learn**: Zheng, X., et al. (2020). "causal-learn: Causal Discovery in Python"

### Dataset

- **IHDP**: Hill, J. L. (2011). "Bayesian Nonparametric Modeling for Causal Inference." _Journal of Computational and Graphical Statistics_.

### Framework

If you use DCRF in your research, please cite:

```bibtex
@software{dcrf2026,
  title={Dynamic Causal Reasoning Framework},
  author={DCRF Development Team},
  year={2026},
  url={https://github.com/your-repo/DCRF}
}
```

---

## Key Concepts Summary

### Why This Framework Matters

1. **Comprehensive**: Covers entire pipeline from discovery to validation
2. **Principled**: Uses causal graphs (not just correlations)
3. **Practical**: Balances statistical rigor with usability
4. **Validated**: Multiple methods, cross-validation, statistical tests
5. **Transparent**: Clear logic, interpretable results, reproducible

### Core Philosophy

**Causal reasoning > Prediction**: Understanding mechanisms, not just fitting data  
**Robustness > Precision**: Multiple methods, conservative estimates, validation  
**Parsimony > Completeness**: Smart selection, 75% coverage, avoid overfitting  
**Temporal constraints**: Leverage time when available - consistency with physics

### Experiment Interdependencies

```
Exp 1 (Impact) ──→ Exp 2 (Quadrants) ──→ Exp 3 (Roles)
                         ↓                      ↓
                    Exp 4 (Selection) ←────────┘
                         ↓
                    Exp 5 (Statistics)
```

Each experiment builds on previous results, forming a complete analysis pipeline.

---

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

## License

This project is available for research and educational purposes. Please cite appropriately if used in publications.

---

## Contact

For questions, issues, or collaboration inquiries, please open an issue in the repository or contact the development team.

---

**Last Updated**: January 2026  
**Version**: 1.0.0
